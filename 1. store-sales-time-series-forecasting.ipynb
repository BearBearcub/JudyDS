{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: pygments in /Users/judymac/Library/Python/3.9/lib/python/site-packages (2.17.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/judymac/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/judymac/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/judymac/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/judymac/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from xgboost) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "# Challange: https://www.kaggle.com/competitions/store-sales-time-series-forecasting\n",
    "\n",
    "#Step 0: intall the required packages\n",
    "!pip install pandas numpy pygments matplotlib seaborn scikit-learn\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing the required libraries\n",
    "# Step 2: Load the data\n",
    "# Step 3: Data Preprocessing\n",
    "# Step 4: Merge the data\n",
    "# Step 5: Check the data types of the merged data\n",
    "# Step 6:  Prediction\n",
    "# Step 7: Model Selection and Tuning\n",
    "# Step 8: Model Evaluation by comparing the models using the best parameters\n",
    "# Step 9: use the extra trees model to predict the sales for the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Importing the required libraries\n",
    "\n",
    "# Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler, PowerTransformer\n",
    "\n",
    "# For model selection and tuning\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "#for sample splitting\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#display formating\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
      "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
      "5  2012-05-12  Holiday     Local        Puyo         Cantonizacion del Puyo   \n",
      "6  2012-06-23  Holiday     Local    Guaranda      Cantonizacion de Guaranda   \n",
      "7  2012-06-25  Holiday  Regional    Imbabura  Provincializacion de Imbabura   \n",
      "8  2012-06-25  Holiday     Local   Latacunga     Cantonizacion de Latacunga   \n",
      "9  2012-06-25  Holiday     Local     Machala           Fundacion de Machala   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n",
      "5        False  \n",
      "6        False  \n",
      "7        False  \n",
      "8        False  \n",
      "9        False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Step 2: Load the data\n",
    "\n",
    "# read the data\n",
    "holidays_events = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/holidays_events.csv')\n",
    "oil = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/oil.csv')\n",
    "stores = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/stores.csv')\n",
    "test = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/test.csv')\n",
    "train = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/train.csv')\n",
    "transactions = pd.read_csv('/Users/judymac/Documents/1. To keep /C. Kaggle data/1. store-sales-time-series-forecasting/transactions.csv')\n",
    "\n",
    "\n",
    "# print the first 10 rows of the data\n",
    "print(holidays_events.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events\n",
      "date           0\n",
      "type           0\n",
      "locale         0\n",
      "locale_name    0\n",
      "description    0\n",
      "transferred    0\n",
      "dtype: int64\n",
      "/n\n",
      "oil\n",
      "date           0\n",
      "dcoilwtico    43\n",
      "dtype: int64\n",
      "/n\n",
      "stores\n",
      "store_nbr    0\n",
      "city         0\n",
      "state        0\n",
      "type         0\n",
      "cluster      0\n",
      "dtype: int64\n",
      "/n\n",
      "test\n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "family         0\n",
      "onpromotion    0\n",
      "dtype: int64\n",
      "/n\n",
      "train\n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "family         0\n",
      "sales          0\n",
      "onpromotion    0\n",
      "dtype: int64\n",
      "/n\n",
      "transactions\n",
      "date            0\n",
      "store_nbr       0\n",
      "transactions    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Data Preprocessing\n",
    "\n",
    "# check for missing values\n",
    "print('holidays_events')\n",
    "print(holidays_events.isnull().sum())\n",
    "print('/n')\n",
    "print('oil')\n",
    "print(oil.isnull().sum())\n",
    "print('/n')\n",
    "print('stores')\n",
    "print(stores.isnull().sum())\n",
    "print('/n')\n",
    "print('test')\n",
    "print(test.isnull().sum())\n",
    "print('/n')\n",
    "print('train')\n",
    "print(train.isnull().sum())\n",
    "print('/n')\n",
    "print('transactions')\n",
    "print(transactions.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holidays_events\n",
      "0\n",
      "/n\n",
      "oil\n",
      "0\n",
      "/n\n",
      "stores\n",
      "0\n",
      "/n\n",
      "test\n",
      "0\n",
      "/n\n",
      "train\n",
      "0\n",
      "/n\n",
      "transactions\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "print('holidays_events')\n",
    "print(holidays_events.duplicated().sum())\n",
    "print('/n')\n",
    "print('oil')\n",
    "print(oil.duplicated().sum())\n",
    "print('/n')\n",
    "print('stores')\n",
    "print(stores.duplicated().sum())\n",
    "print('/n')\n",
    "print('test')\n",
    "print(test.duplicated().sum())\n",
    "print('/n')\n",
    "print('train')\n",
    "print(train.duplicated().sum())\n",
    "print('/n')\n",
    "print('transactions')\n",
    "print(transactions.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil null values\n",
      "            date  dcoilwtico\n",
      "0     2013-01-01         NaN\n",
      "14    2013-01-21         NaN\n",
      "34    2013-02-18         NaN\n",
      "63    2013-03-29         NaN\n",
      "104   2013-05-27         NaN\n",
      "132   2013-07-04         NaN\n",
      "174   2013-09-02         NaN\n",
      "237   2013-11-28         NaN\n",
      "256   2013-12-25         NaN\n",
      "261   2014-01-01         NaN\n",
      "274   2014-01-20         NaN\n",
      "294   2014-02-17         NaN\n",
      "338   2014-04-18         NaN\n",
      "364   2014-05-26         NaN\n",
      "393   2014-07-04         NaN\n",
      "434   2014-09-01         NaN\n",
      "497   2014-11-27         NaN\n",
      "517   2014-12-25         NaN\n",
      "522   2015-01-01         NaN\n",
      "534   2015-01-19         NaN\n",
      "554   2015-02-16         NaN\n",
      "588   2015-04-03         NaN\n",
      "624   2015-05-25         NaN\n",
      "653   2015-07-03         NaN\n",
      "699   2015-09-07         NaN\n",
      "757   2015-11-26         NaN\n",
      "778   2015-12-25         NaN\n",
      "783   2016-01-01         NaN\n",
      "794   2016-01-18         NaN\n",
      "814   2016-02-15         NaN\n",
      "843   2016-03-25         NaN\n",
      "889   2016-05-30         NaN\n",
      "914   2016-07-04         NaN\n",
      "959   2016-09-05         NaN\n",
      "1017  2016-11-24         NaN\n",
      "1039  2016-12-26         NaN\n",
      "1044  2017-01-02         NaN\n",
      "1054  2017-01-16         NaN\n",
      "1079  2017-02-20         NaN\n",
      "1118  2017-04-14         NaN\n",
      "1149  2017-05-29         NaN\n",
      "1174  2017-07-03         NaN\n",
      "1175  2017-07-04         NaN\n"
     ]
    }
   ],
   "source": [
    "# show null values in the data\n",
    "print('oil null values')\n",
    "print(oil[oil.isnull().any(axis=1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date          0\n",
      "dcoilwtico    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#replace missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "oil['dcoilwtico'] = imputer.fit_transform(oil[['dcoilwtico']])\n",
    "print(oil.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id        date  store_nbr        family  sales  onpromotion   city  \\\n",
      "0   0  2013-01-01          1    AUTOMOTIVE    0.0            0  Quito   \n",
      "1   1  2013-01-01          1     BABY CARE    0.0            0  Quito   \n",
      "2   2  2013-01-01          1        BEAUTY    0.0            0  Quito   \n",
      "3   3  2013-01-01          1     BEVERAGES    0.0            0  Quito   \n",
      "4   4  2013-01-01          1         BOOKS    0.0            0  Quito   \n",
      "5   5  2013-01-01          1  BREAD/BAKERY    0.0            0  Quito   \n",
      "6   6  2013-01-01          1   CELEBRATION    0.0            0  Quito   \n",
      "7   7  2013-01-01          1      CLEANING    0.0            0  Quito   \n",
      "8   8  2013-01-01          1         DAIRY    0.0            0  Quito   \n",
      "9   9  2013-01-01          1          DELI    0.0            0  Quito   \n",
      "\n",
      "       state type_stores  cluster  dcoilwtico type_holidays_events    locale  \\\n",
      "0  Pichincha           D       13   67.714366              Holiday  National   \n",
      "1  Pichincha           D       13   67.714366              Holiday  National   \n",
      "2  Pichincha           D       13   67.714366              Holiday  National   \n",
      "3  Pichincha           D       13   67.714366              Holiday  National   \n",
      "4  Pichincha           D       13   67.714366              Holiday  National   \n",
      "5  Pichincha           D       13   67.714366              Holiday  National   \n",
      "6  Pichincha           D       13   67.714366              Holiday  National   \n",
      "7  Pichincha           D       13   67.714366              Holiday  National   \n",
      "8  Pichincha           D       13   67.714366              Holiday  National   \n",
      "9  Pichincha           D       13   67.714366              Holiday  National   \n",
      "\n",
      "  locale_name         description  transferred  \n",
      "0     Ecuador  Primer dia del ano        False  \n",
      "1     Ecuador  Primer dia del ano        False  \n",
      "2     Ecuador  Primer dia del ano        False  \n",
      "3     Ecuador  Primer dia del ano        False  \n",
      "4     Ecuador  Primer dia del ano        False  \n",
      "5     Ecuador  Primer dia del ano        False  \n",
      "6     Ecuador  Primer dia del ano        False  \n",
      "7     Ecuador  Primer dia del ano        False  \n",
      "8     Ecuador  Primer dia del ano        False  \n",
      "9     Ecuador  Primer dia del ano        False  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: merge the data\n",
    "\n",
    "# merge the train data and rename it to train_df, use inner to avoid null values\n",
    "train_df = pd.merge(train, stores.rename(columns={'type': 'type_stores'}), how='inner', on='store_nbr')\n",
    "train_df = pd.merge(train_df, oil.rename(columns={'type': 'type_oil'}), how='inner', on='date')\n",
    "# train_df = pd.merge(train_df, transactions.rename(columns={'type': 'type_transacti\n",
    "# no details about the transcactions data from the database (https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=transactions.csv)\n",
    "train_df = pd.merge(train_df, holidays_events.rename(columns={'type': 'type_holidays_events'}), how='inner', on='date')\n",
    "\n",
    "#print the first 10 rows of the merged data\n",
    "print(train_df.head(10))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id        date  store_nbr        family  onpromotion   city  \\\n",
      "0  3015144  2017-08-24          1    AUTOMOTIVE            0  Quito   \n",
      "1  3015145  2017-08-24          1     BABY CARE            0  Quito   \n",
      "2  3015146  2017-08-24          1        BEAUTY            0  Quito   \n",
      "3  3015147  2017-08-24          1     BEVERAGES           26  Quito   \n",
      "4  3015148  2017-08-24          1         BOOKS            0  Quito   \n",
      "5  3015149  2017-08-24          1  BREAD/BAKERY            1  Quito   \n",
      "6  3015150  2017-08-24          1   CELEBRATION            0  Quito   \n",
      "7  3015151  2017-08-24          1      CLEANING           12  Quito   \n",
      "8  3015152  2017-08-24          1         DAIRY           13  Quito   \n",
      "9  3015153  2017-08-24          1          DELI           11  Quito   \n",
      "\n",
      "       state type_stores  cluster  dcoilwtico type_holidays_events locale  \\\n",
      "0  Pichincha           D       13       47.24              Holiday  Local   \n",
      "1  Pichincha           D       13       47.24              Holiday  Local   \n",
      "2  Pichincha           D       13       47.24              Holiday  Local   \n",
      "3  Pichincha           D       13       47.24              Holiday  Local   \n",
      "4  Pichincha           D       13       47.24              Holiday  Local   \n",
      "5  Pichincha           D       13       47.24              Holiday  Local   \n",
      "6  Pichincha           D       13       47.24              Holiday  Local   \n",
      "7  Pichincha           D       13       47.24              Holiday  Local   \n",
      "8  Pichincha           D       13       47.24              Holiday  Local   \n",
      "9  Pichincha           D       13       47.24              Holiday  Local   \n",
      "\n",
      "  locale_name          description  transferred  \n",
      "0      Ambato  Fundacion de Ambato        False  \n",
      "1      Ambato  Fundacion de Ambato        False  \n",
      "2      Ambato  Fundacion de Ambato        False  \n",
      "3      Ambato  Fundacion de Ambato        False  \n",
      "4      Ambato  Fundacion de Ambato        False  \n",
      "5      Ambato  Fundacion de Ambato        False  \n",
      "6      Ambato  Fundacion de Ambato        False  \n",
      "7      Ambato  Fundacion de Ambato        False  \n",
      "8      Ambato  Fundacion de Ambato        False  \n",
      "9      Ambato  Fundacion de Ambato        False  \n"
     ]
    }
   ],
   "source": [
    "# merge the test data and rename it to test_df, use inner to avoid null values\n",
    "test_df = pd.merge(test, stores.rename(columns={'type': 'type_stores'}), how='inner', on='store_nbr')\n",
    "test_df = pd.merge(test_df, oil.rename(columns={'type': 'type_oil'}), how='inner', on='date')\n",
    "# test_df = pd.merge(test_df, transactions.rename(columns={'type': 'type_transactions'}), how='inner', on=['date', 'store_nbr'])\n",
    "test_df = pd.merge(test_df, holidays_events.rename(columns={'type': 'type_holidays_events'}), how='inner', on='date')\n",
    "\n",
    "# #print the first 10 rows of the merged data\n",
    "print(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df data types\n",
      "id                        int64\n",
      "date                     object\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "sales                   float64\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 5: check the data types of the merged data\n",
    "# show the data types of the merged data\n",
    "print('train_df data types')\n",
    "print(train_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df data types\n",
      "id                        int64\n",
      "date                     object\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('test_df data types')\n",
    "print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df size\n",
      "(352836, 16)\n",
      "test_df size\n",
      "(1782, 15)\n"
     ]
    }
   ],
   "source": [
    "# Show the size of train_df and test_df\n",
    "\n",
    "print('train_df size')\n",
    "print(train_df.shape)\n",
    "\n",
    "print('test_df size')\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6: prediction\n",
    "\n",
    "\n",
    "\n",
    "# create a dictionary of models, including all popular models like 'Linear', 'Ridge', 'Lasso', 'ElasticNet', 'Extra Tree', 'Gradient Boosting', 'XGradientBoosting', 'DecisionTree', 'KNeighbors'\n",
    "models = {\n",
    "\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Extra Tree': ExtraTreesRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'XGradientBoosting': XGBRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor()\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df data types\n",
      "id                        int64\n",
      "date                     object\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "sales                   float64\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n",
      "test_df data types\n",
      "id                        int64\n",
      "date                     object\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# see data types of the data\n",
    "\n",
    "print('train_df data types')\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print('test_df data types')\n",
    "print(test_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    734869\n",
      "1    734869\n",
      "2    734869\n",
      "3    734869\n",
      "4    734869\n",
      "5    734869\n",
      "6    734869\n",
      "7    734869\n",
      "8    734869\n",
      "9    734869\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# conconvet date to float\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "train_df['date'] = train_df['date'].map(datetime.datetime.toordinal)\n",
    "\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "test_df['date'] = test_df['date'].map(datetime.datetime.toordinal)\n",
    "\n",
    "print(train_df['date'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('sales', axis=1)\n",
    "y_train = train_df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical columns for train_df and test_df\n",
    "train_categorical_cols = X_train.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "\n",
    "# get numerical columns for train_df and test_df, include int64 and float64\n",
    "train_numerical_cols = X_train.select_dtypes(include=[np.int64, np.float64]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'store_nbr', 'family', 'sales', 'onpromotion', 'city',\n",
      "       'state', 'type_stores', 'cluster', 'dcoilwtico', 'type_holidays_events',\n",
      "       'locale', 'locale_name', 'description', 'transferred'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# use sk learn ColumnTransformer to transform the data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create a column transformer for the train data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), train_numerical_cols),\n",
    "        ('cat', OneHotEncoder(), train_categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Extra Tree': ExtraTreesRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'XGradientBoosting': XGBRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor()    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Extra Tree': ExtraTreesRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'XGradientBoosting': XGBRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    0.0\n",
      "6    0.0\n",
      "7    0.0\n",
      "8    0.0\n",
      "9    0.0\n",
      "Name: sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(X_train.head(10))\n",
    "print(y_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df null values\n",
      "id                      0\n",
      "date                    0\n",
      "store_nbr               0\n",
      "family                  0\n",
      "sales                   0\n",
      "onpromotion             0\n",
      "city                    0\n",
      "state                   0\n",
      "type_stores             0\n",
      "cluster                 0\n",
      "dcoilwtico              0\n",
      "type_holidays_events    0\n",
      "locale                  0\n",
      "locale_name             0\n",
      "description             0\n",
      "transferred             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show null values in the data\n",
    "\n",
    "print('train_df null values')\n",
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the train_df to train_full_df\n",
    "\n",
    "train_full_df = train_df.copy()\n",
    "\n",
    "# sample 10% of the train_full_df as train_df\n",
    "train_df = train_full_df.sample(frac=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  {'alpha': 10}\n",
      "Best score:  0.5085249054912283\n",
      "Best estimator:  Ridge(alpha=10)\n",
      "Best index:  4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming 'sales' is the target variable and train_df is your original data\n",
    "X_train_orig = train_df.drop('sales', axis=1)\n",
    "y_train = train_df['sales']\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train = preprocessor.fit_transform(X_train_orig)\n",
    "\n",
    "#step 7: Model Selection and Tuning\n",
    "\n",
    "#step 7.1 Ridge Regression\n",
    "\n",
    "# Initialize a Ridge regressor\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the grid of values for alpha\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha value\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=5)  # 5-fold cross-validation\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha: \", ridge_cv.best_params_)\n",
    "print(\"Best score: \", ridge_cv.best_score_)\n",
    "print(\"Best estimator: \", ridge_cv.best_estimator_)\n",
    "print(\"Best index: \", ridge_cv.best_index_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 177063891.07149887, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 249243130.16078568, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 226099264.45410156, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 656671811.02285, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 610267191.9098778, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso:  {'alpha': 0.1}\n",
      "Best score for Lasso:  0.5086569616946288\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Lasso Regression\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize a Lasso regressor\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define a simplified grid of values for alpha\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100]  # Reduced range of alpha values\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha value\n",
    "lasso_cv = GridSearchCV(lasso, param_grid_lasso, cv=5, n_jobs=-1)  # 5-fold cross-validation and parallel processing\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for Lasso: \", lasso_cv.best_params_)\n",
    "print(\"Best score for Lasso: \", lasso_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5327783756.427832, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1993502462.9433384, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11227326147.460999, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9380671002.491304, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8074647603.3091955, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9354406689.427544, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1131242773.8038902, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11438923505.265495, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1862143720.5207233, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4698730948.052752, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7170275489.938013, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10524425164.934292, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6820549718.634175, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5085866631.085335, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6110144349.014671, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6423127725.493614, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11330111031.20918, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7829696890.721125, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7726673551.087698, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5799527479.52602, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10055751607.715637, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8232000889.491134, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8039429216.973801, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11111258367.080647, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7526916737.711701, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 177063891.07149887, tolerance: 3928325.407710803\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 610267191.9098778, tolerance: 4484630.909622386\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 656671811.02285, tolerance: 4702899.542180308\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 226099264.45410156, tolerance: 4399082.4097044775\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:639: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 249243130.16078568, tolerance: 4639734.320645217\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for ElasticNet:  {'alpha': 0.1, 'l1_ratio': 1}\n",
      "Best score for ElasticNet:  0.5086569616946288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.3 ElasticNet Regression\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize an ElasticNet regressor\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Define the grid of values for alpha and l1_ratio\n",
    "param_grid_elastic_net = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9, 1]  # l1_ratio is the mix ratio between Lasso and Ridge\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha and l1_ratio values\n",
    "elastic_net_cv = GridSearchCV(elastic_net, param_grid_elastic_net, cv=5, n_jobs=-1)  # 5-fold cross-validation and parallel processing\n",
    "elastic_net_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters for ElasticNet: \", elastic_net_cv.best_params_)\n",
    "print(\"Best score for ElasticNet: \", elastic_net_cv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for ExtraTreesRegressor:  {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best score for ExtraTreesRegressor:  0.6429973935685289\n"
     ]
    }
   ],
   "source": [
    "# 7.4 ExtraTreesRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize ExtraTreesRegressor\n",
    "extra_trees = ExtraTreesRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid_extra_trees = {\n",
    "    'n_estimators': [50, 100],        # Number of trees in the forest\n",
    "    'max_features': ['sqrt'],         # Number of features to consider at each split\n",
    "    'max_depth': [10, 20],            # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],      # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]        # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "extra_trees_cv = GridSearchCV(extra_trees, param_grid_extra_trees, cv=5, n_jobs=-1)\n",
    "extra_trees_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for ExtraTreesRegressor: \", extra_trees_cv.best_params_)\n",
    "print(\"Best score for ExtraTreesRegressor: \", extra_trees_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for GradientBoostingRegressor:  {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best score for GradientBoostingRegressor:  0.702108281843524\n"
     ]
    }
   ],
   "source": [
    "# 7.5 GradientBoostingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [50, 100],        # Number of boosting stages to be run\n",
    "    'learning_rate': [0.01, 0.1],     # Step size shrinkage used in update to prevent overfitting\n",
    "    'max_depth': [3, 5],              # Maximum depth of the individual regression estimators\n",
    "    'min_samples_split': [2, 5],      # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]        # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "gbr_cv = GridSearchCV(gbr, param_grid_gbr, cv=5, n_jobs=-1)\n",
    "gbr_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for GradientBoostingRegressor: \", gbr_cv.best_params_)\n",
    "print(\"Best score for GradientBoostingRegressor: \", gbr_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBRegressor:  {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 100}\n",
      "Best score for XGBRegressor:  0.705338934341343\n"
     ]
    }
   ],
   "source": [
    "# 7.6 XGBRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100],        # Number of boosting stages to be run\n",
    "    'learning_rate': [0.01, 0.1],     # Step size shrinkage used in update to prevent overfitting\n",
    "    'max_depth': [3, 5],              # Maximum depth of the individual regression estimators\n",
    "    'min_child_weight': [1, 2]        # Minimum sum of instance weight needed in a child\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "xgb_cv = GridSearchCV(xgb, param_grid_xgb, cv=5, n_jobs=-1)\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for XGBRegressor: \", xgb_cv.best_params_)\n",
    "print(\"Best score for XGBRegressor: \", xgb_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for DecisionTreeRegressor:  {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Best score for DecisionTreeRegressor:  0.491075122048448\n"
     ]
    }
   ],
   "source": [
    "#7.7 DecisionTreeRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize DecisionTreeRegressor\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid_decision_tree = {\n",
    "    'max_depth': [3, 5],              # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],      # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]        # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "decision_tree_cv = GridSearchCV(decision_tree, param_grid_decision_tree, cv=5, n_jobs=-1)\n",
    "decision_tree_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for DecisionTreeRegressor: \", decision_tree_cv.best_params_)\n",
    "print(\"Best score for DecisionTreeRegressor: \", decision_tree_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNeighborsRegressor:  {'n_neighbors': 7, 'weights': 'distance'}\n",
      "Best score for KNeighborsRegressor:  0.378755980547664\n"
     ]
    }
   ],
   "source": [
    "# 7.8 KNeighborsRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],          # Number of neighbors to use\n",
    "    'weights': ['uniform', 'distance']  # Weight function used in prediction\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "knn_cv = GridSearchCV(knn, param_grid_knn, cv=5, n_jobs=-1)\n",
    "\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for KNeighborsRegressor: \", knn_cv.best_params_)\n",
    "print(\"Best score for KNeighborsRegressor: \", knn_cv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge:  {'alpha': 10}\n",
      "Best score for Ridge:  0.5085249054912283\n",
      "Best parameters for Lasso:  {'alpha': 0.1}\n",
      "Best score for Lasso:  0.5086569616946288\n",
      "Best parameters for ElasticNet:  {'alpha': 0.1, 'l1_ratio': 1}\n",
      "Best score for ElasticNet:  0.5086569616946288\n",
      "Best parameters for ExtraTreesRegressor:  {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best score for ExtraTreesRegressor:  0.6429973935685289\n",
      "Best parameters for GradientBoostingRegressor:  {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best score for GradientBoostingRegressor:  0.702108281843524\n",
      "Best parameters for XGBRegressor:  {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 100}\n",
      "Best score for XGBRegressor:  0.705338934341343\n",
      "Best parameters for DecisionTreeRegressor:  {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Best score for DecisionTreeRegressor:  0.491075122048448\n",
      "Best parameters for KNeighborsRegressor:  {'n_neighbors': 7, 'weights': 'distance'}\n",
      "Best score for KNeighborsRegressor:  0.378755980547664\n"
     ]
    }
   ],
   "source": [
    "# print the best parameters and best score for all the models\n",
    "\n",
    "print(\"Best parameters for Ridge: \", ridge_cv.best_params_)\n",
    "print(\"Best score for Ridge: \", ridge_cv.best_score_)\n",
    "print(\"Best parameters for Lasso: \", lasso_cv.best_params_)\n",
    "print(\"Best score for Lasso: \", lasso_cv.best_score_)\n",
    "print(\"Best parameters for ElasticNet: \", elastic_net_cv.best_params_)\n",
    "print(\"Best score for ElasticNet: \", elastic_net_cv.best_score_)\n",
    "print(\"Best parameters for ExtraTreesRegressor: \", extra_trees_cv.best_params_)\n",
    "print(\"Best score for ExtraTreesRegressor: \", extra_trees_cv.best_score_)\n",
    "print(\"Best parameters for GradientBoostingRegressor: \", gbr_cv.best_params_)\n",
    "print(\"Best score for GradientBoostingRegressor: \", gbr_cv.best_score_)\n",
    "print(\"Best parameters for XGBRegressor: \", xgb_cv.best_params_)\n",
    "print(\"Best score for XGBRegressor: \", xgb_cv.best_score_)\n",
    "print(\"Best parameters for DecisionTreeRegressor: \", decision_tree_cv.best_params_)\n",
    "print(\"Best score for DecisionTreeRegressor: \", decision_tree_cv.best_score_)\n",
    "print(\"Best parameters for KNeighborsRegressor: \", knn_cv.best_params_)\n",
    "print(\"Best score for KNeighborsRegressor: \", knn_cv.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 Model Evaluation by comparing the models using the best parameters\n",
    "\n",
    "# 8.1 Create a dictionary of models with the best parameters\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=0.01),\n",
    "    'Lasso': Lasso(alpha=0.01),\n",
    "    'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.1),\n",
    "    'Extra Trees': ExtraTreesRegressor(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(learning_rate=0.1, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, min_samples_leaf=1, min_samples_split=2),\n",
    "    'KNeighbors': KNeighborsRegressor(n_neighbors=7, weights='distance')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 use another 10% of the data to evaluate the models and rename the database as evaluation_df\n",
    "\n",
    "evaluation_df = train_full_df.sample(frac=0.1, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_df data types\n",
      "id                        int64\n",
      "date                      int64\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "sales                   float64\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n",
      "train_df data types\n",
      "id                        int64\n",
      "date                      int64\n",
      "store_nbr                 int64\n",
      "family                   object\n",
      "sales                   float64\n",
      "onpromotion               int64\n",
      "city                     object\n",
      "state                    object\n",
      "type_stores              object\n",
      "cluster                   int64\n",
      "dcoilwtico              float64\n",
      "type_holidays_events     object\n",
      "locale                   object\n",
      "locale_name              object\n",
      "description              object\n",
      "transferred                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 8.3 show the evaluation_df type\n",
    "print('evaluation_df data types')\n",
    "print(evaluation_df.dtypes)\n",
    "\n",
    "# show the train_df type\n",
    "\n",
    "print('train_df data types')\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ridge\n",
      "R^2: 0.5921\n",
      "RMSE: 725.69\n",
      "MSE: 526619.47\n",
      "MAE: 315.27\n",
      "\n",
      "Model: Lasso\n",
      "R^2: 0.5921\n",
      "RMSE: 725.67\n",
      "MSE: 526592.07\n",
      "MAE: 315.03\n",
      "\n",
      "Model: ElasticNet\n",
      "R^2: 0.5717\n",
      "RMSE: 743.58\n",
      "MSE: 552910.89\n",
      "MAE: 297.15\n",
      "\n",
      "Model: Extra Trees\n",
      "R^2: 0.7814\n",
      "RMSE: 531.19\n",
      "MSE: 282163.46\n",
      "MAE: 209.50\n",
      "\n",
      "Model: Gradient Boosting\n",
      "R^2: 0.8492\n",
      "RMSE: 441.17\n",
      "MSE: 194635.32\n",
      "MAE: 152.79\n",
      "\n",
      "Model: XGBoost\n",
      "R^2: 0.8320\n",
      "RMSE: 465.72\n",
      "MSE: 216893.53\n",
      "MAE: 154.93\n",
      "\n",
      "Model: Decision Tree\n",
      "R^2: 0.6705\n",
      "RMSE: 652.20\n",
      "MSE: 425359.72\n",
      "MAE: 247.09\n",
      "\n",
      "Model: KNeighbors\n",
      "R^2: 0.5293\n",
      "RMSE: 779.52\n",
      "MSE: 607648.64\n",
      "MAE: 296.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the evaluation_df to evaluate the models to get the r2, rmse, mse, and mae. Show the result with best R2 score\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Preprocess the evaluation data\n",
    "    X_eval = evaluation_df.drop('sales', axis=1)\n",
    "    y_eval = evaluation_df['sales']\n",
    "    X_eval = preprocessor.transform(X_eval)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_eval)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_eval, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_eval, y_pred))\n",
    "    mse = mean_squared_error(y_eval, y_pred)\n",
    "    mae = mean_absolute_error(y_eval, y_pred)\n",
    "    \n",
    "    # Print the metrics\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"R^2: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id        sales\n",
      "0  3015144   150.585198\n",
      "1  3015145   114.335088\n",
      "2  3015146   118.413302\n",
      "3  3015147  3304.529703\n",
      "4  3015148   185.077539\n"
     ]
    }
   ],
   "source": [
    "# Step 9: use the extra trees model to predict the sales for the test data\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test = test_df\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Fit the Extra Trees model\n",
    "extra_trees = ExtraTreesRegressor(max_depth=20, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "extra_trees.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sales\n",
    "y_pred_test = extra_trees.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'sales': y_pred_test})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('store_sales_time_series_forecasting_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "\n",
    "print(predictions_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
